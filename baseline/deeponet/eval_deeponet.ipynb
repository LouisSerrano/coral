{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kassai/code/coral/dynamics_modeling/train.py:51: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"config/\", config_name=\"ode.yaml\")\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import einops\n",
    "import yaml\n",
    "\n",
    "from coral.utils.data.load_data import (set_seed, get_dynamics_data)\n",
    "from coral.utils.data.dynamics_dataset import TemporalDatasetWithCode, KEY_TO_INDEX\n",
    "from dynamics_modeling.train import DetailedMSE\n",
    "from deeponet import DeepONet, AR_forward\n",
    "from eval import eval_deeponet\n",
    "from forwards_operator import forward_deeponet_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DictConfig(yaml.safe_load(open(\"config/deeponet.yaml\")))\n",
    "dataset_name = cfg.data.dataset_name\n",
    "dataset_name = 'navier-stokes-dino'\n",
    "run_name = cfg.deeponet.run_name\n",
    "run_name='lilac-star-5018'\n",
    "root_dir = Path(os.getenv(\"WANDB_DIR\")) / dataset_name\n",
    "\n",
    "tmp = torch.load(root_dir / \"deeponet\" / f\"{run_name}_tr.pt\")\n",
    "cfg = tmp['cfg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_dir = cfg.data.dir\n",
    "dataset_name = cfg.data.dataset_name\n",
    "ntrain = cfg.data.ntrain\n",
    "ntest = cfg.data.ntest\n",
    "data_to_encode = cfg.data.data_to_encode\n",
    "sub_from = 4\n",
    "sub_tr = cfg.data.sub_tr\n",
    "sub_te = cfg.data.sub_te\n",
    "seed = cfg.data.seed\n",
    "same_grid = cfg.data.same_grid\n",
    "seq_inter_len = cfg.data.seq_inter_len\n",
    "seq_extra_len = cfg.data.seq_extra_len\n",
    "\n",
    "# deeponet\n",
    "model_type = cfg.deeponet.model_type\n",
    "code_dim = 1\n",
    "branch_depth = cfg.deeponet.branch_depth\n",
    "trunk_depth = cfg.deeponet.trunk_depth\n",
    "width = cfg.deeponet.width\n",
    "\n",
    "# optim\n",
    "batch_size = cfg.optim.batch_size\n",
    "batch_size_val = (\n",
    "    batch_size if cfg.optim.batch_size_val == None else cfg.optim.batch_size_val\n",
    ")\n",
    "\n",
    "multichannel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_tr, sub_from, sub_te :  0.2 4 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"sub_tr, sub_from, sub_te : \", sub_tr, sub_from, sub_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: navier-stokes-dino, u_train: torch.Size([256, 819, 1, 20]), u_train_eval: torch.Size([256, 819, 1, 40]), u_test: torch.Size([16, 819, 1, 40])\n",
      "grid: grid_tr: torch.Size([256, 819, 2, 20]), grid_tr_extra: torch.Size([256, 819, 2, 40]), grid_te: torch.Size([16, 819, 2, 40])\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "(u_train, u_eval_extrapolation, u_test, grid_tr, grid_tr_extra, grid_te) = get_dynamics_data(\n",
    "    data_dir,\n",
    "    dataset_name,\n",
    "    ntrain,\n",
    "    ntest,\n",
    "    seq_inter_len=seq_inter_len,\n",
    "    seq_extra_len=seq_extra_len,\n",
    "    sub_from=sub_from,\n",
    "    sub_tr=sub_tr,\n",
    "    sub_te=sub_te,\n",
    "    same_grid=same_grid,\n",
    ")\n",
    "\n",
    "# (_, _, u_test_2, _, _, grid_te_2) = get_dynamics_data(\n",
    "#     data_dir,\n",
    "#     dataset_name,\n",
    "#     ntrain,\n",
    "#     ntest,\n",
    "#     seq_inter_len=seq_inter_len,\n",
    "#     seq_extra_len=seq_extra_len,\n",
    "#     sub_tr=sub_tr,\n",
    "#     sub_te=sub_te_2,\n",
    "#     same_grid=same_grid,\n",
    "# )\n",
    "\n",
    "# flatten spatial dims\n",
    "u_train = einops.rearrange(u_train, 'B ... C T -> B (...) C T')\n",
    "grid_tr = einops.rearrange(grid_tr, 'B ... C T -> B (...) C T')  # * 0.5\n",
    "u_test = einops.rearrange(u_test, 'B ... C T -> B (...) C T')\n",
    "grid_te = einops.rearrange(grid_te, 'B ... C T -> B (...) C T')  # * 0.5\n",
    "u_eval_extrapolation = einops.rearrange(u_eval_extrapolation, 'B ... C T -> B (...) C T')\n",
    "grid_tr_extra = einops.rearrange(grid_tr_extra, 'B ... C T -> B (...) C T')  # * 0.5\n",
    "\n",
    "# u_test_2 = einops.rearrange(u_test_2, 'B ... C T -> B (...) C T')\n",
    "# grid_te_2 = einops.rearrange(grid_te_2, 'B ... C T -> B (...) C T')  # * 0.5\n",
    "\n",
    "print(f\"data: {dataset_name}, u_train: {u_train.shape}, u_train_eval: {u_eval_extrapolation.shape}, u_test: {u_test.shape}\")\n",
    "print(f\"grid: grid_tr: {grid_tr.shape}, grid_tr_extra: {grid_tr_extra.shape}, grid_te: {grid_te.shape}\")\n",
    "\n",
    "n_seq_train = u_train.shape[0]  # 512 en dur\n",
    "n_seq_test = u_test.shape[0]  # 512 en dur\n",
    "spatial_size = u_train.shape[1] * u_train.shape[2] # 64 en dur\n",
    "state_dim = u_train.shape[2]  # N, XY, C, T\n",
    "coord_dim = grid_tr.shape[2]  # N, XY, C, T\n",
    "T = u_train.shape[-1]\n",
    "\n",
    "ntrain = u_train.shape[0]  # int(u_train.shape[0]*T)\n",
    "ntest = u_test.shape[0]  # int(u_test.shape[0]*T)\n",
    "\n",
    "\n",
    "trainset_out = TemporalDatasetWithCode(\n",
    "    u_eval_extrapolation, grid_tr_extra, code_dim, dataset_name, data_to_encode\n",
    ")\n",
    "\n",
    "testset = TemporalDatasetWithCode(\n",
    "    u_test, grid_te, code_dim, dataset_name, data_to_encode\n",
    ")\n",
    "\n",
    "# testset_2 = TemporalDatasetWithCode(\n",
    "#     u_test_2, grid_te_2, code_dim, dataset_name, data_to_encode\n",
    "# )\n",
    "\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    trainset_out,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # TODO : here shuffle to False because error cuda (?!)\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size_val,\n",
    "    shuffle=True, # TODO : here shuffle to False because error cuda (?!)\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# test_loader_2 = torch.utils.data.DataLoader(\n",
    "#     testset_2,\n",
    "#     batch_size=batch_size_val,\n",
    "#     shuffle=True, # TODO : here shuffle to False because error cuda (?!)\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "if multichannel:\n",
    "    detailed_train_eval_mse = DetailedMSE(list(KEY_TO_INDEX[dataset_name].keys()),\n",
    "                                            dataset_name,\n",
    "                                            mode=\"train_extra\",\n",
    "                                            n_trajectories=n_seq_train)\n",
    "    detailed_test_mse = DetailedMSE(list(KEY_TO_INDEX[dataset_name].keys()),\n",
    "                                    dataset_name,\n",
    "                                    mode=\"test\",\n",
    "                                    n_trajectories=n_seq_test)\n",
    "else:\n",
    "    detailed_train_eval_mse = None\n",
    "    detailed_test_mse = None\n",
    "\n",
    "T = u_train.shape[-1]\n",
    "T_EXT = u_test.shape[-1]\n",
    "\n",
    "dt = 1\n",
    "timestamps_train = torch.arange(0, T, dt).float().cuda()\n",
    "timestamps_ext = torch.arange(0, T_EXT, dt).float().cuda()\n",
    "\n",
    "net_dyn_params = {\n",
    "    'branch_dim': spatial_size,\n",
    "    'branch_depth': branch_depth,\n",
    "    'trunk_dim': coord_dim,\n",
    "    'trunk_depth': trunk_depth,\n",
    "    'width': width\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeponet = DeepONet(**net_dyn_params, logger=None, input_dataset=dataset_name)\n",
    "deeponet.load_state_dict(tmp['deeponet_state_dict'])\n",
    "deeponet = deeponet.to('cuda')\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating train...\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([40, 40, 819, 1])\n",
      "torch.Size([16, 40, 819, 1])\n",
      "Evaluating test...\n",
      "torch.Size([16, 40, 819, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating train...\")\n",
    "(\n",
    "    pred_train_mse,\n",
    "    pred_train_inter_mse,\n",
    "    pred_train_extra_mse\n",
    ") = eval_deeponet(\n",
    "    deeponet, \n",
    "    valid_loader,\n",
    "    'cuda', \n",
    "    timestamps_ext,\n",
    "    criterion, \n",
    "    n_seq_train, \n",
    "    seq_inter_len, \n",
    "    seq_extra_len, \n",
    "    detailed_train_eval_mse\n",
    ")\n",
    "\n",
    "# Out-of-domain evaluation\n",
    "print(\"Evaluating test...\")\n",
    "(\n",
    "    pred_test_mse,\n",
    "    pred_test_inter_mse,\n",
    "    pred_test_extra_mse\n",
    ") = eval_deeponet(\n",
    "    deeponet, \n",
    "    test_loader, \n",
    "    'cuda', \n",
    "    timestamps_ext,\n",
    "    criterion, \n",
    "    n_seq_test, \n",
    "    seq_inter_len, \n",
    "    seq_extra_len, \n",
    "    detailed_test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_train_inter_mse :  0.0313153974711895\n",
      "pred_train_extra_mse : 0.031304750591516495\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_train_inter_mse : \", pred_train_inter_mse)\n",
    "print('pred_train_extra_mse :' , pred_train_extra_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_test_inter_mse :  0.5221408009529114\n",
      "pred_test_extra_mse : 0.5004575848579407\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_test_inter_mse : \", pred_test_inter_mse)\n",
    "print('pred_test_extra_mse :' , pred_test_extra_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_deeponet_up(deeponet, dataloader, testset, device, timestamps, criterion, n_seq, n_frames_in, n_frames_out, detailed_mse, multichannel=False):\n",
    "    \"\"\"def eval_dino(\n",
    "    dataloader,\n",
    "    net_dyn,\n",
    "    net_dec,\n",
    "    device,\n",
    "    method,\n",
    "    criterion,\n",
    "    state_dim,\n",
    "    code_dim,\n",
    "    coord_dim,\n",
    "    detailed_mse,\n",
    "    timestamps,\n",
    "    n_seq,\n",
    "    n_frames_train=0,\n",
    "    n_frames_test=0,\n",
    "    states_params=None,\n",
    "    lr_adapt=0.0,\n",
    "    n_steps=300,\n",
    "    multichannel=False,\n",
    "    save_best=True,\n",
    "):\"\"\"\n",
    "    \"\"\"\n",
    "    In_t: loss within train horizon.\n",
    "    Out_t: loss outside train horizon.\n",
    "    In_s: loss within observation grid.\n",
    "    Out_s: loss outside observation grid.\n",
    "    loss: loss averaged across in_t/out_t and in_s/out_s\n",
    "    loss_in_t: loss averaged across in_s/out_s for in_t.\n",
    "    loss_in_t_in_s, loss_in_t_out_s: loss in_t + in_s / out_s\n",
    "    \"\"\"\n",
    "\n",
    "    (\n",
    "        loss,\n",
    "        loss_out_t,\n",
    "        loss_in_t,\n",
    "    ) = (0.0, 0.0, 0.0)\n",
    "\n",
    "    set_requires_grad(deeponet, False)\n",
    "\n",
    "    for j, (images, _, coords, idx) in enumerate(dataloader):\n",
    "        # flatten spatial dims\n",
    "        t = timestamps.to(device)\n",
    "        images_up = testset[idx][0] # 1, 4096, 1, 40\n",
    "        coords_up = testset[idx][2] # 1, 4096, 2, 40\n",
    "        ground_truth = einops.rearrange(images, 'B ... C T -> B (...) C T')\n",
    "        model_input = einops.rearrange(coords, 'B ... C T -> B (...) C T')\n",
    "        ground_truth_up = einops.rearrange(images_up, 'B ... C T -> B (...) C T')\n",
    "        model_input_up = einops.rearrange(coords_up, 'B ... C T -> B (...) C T')\n",
    "\n",
    "        # permute axis for forward\n",
    "        ground_truth = torch.permute(\n",
    "            ground_truth, (0, 3, 1, 2)).to(device)  # [B, XY, C, T] -> [B, T, XY, C]\n",
    "        model_input = torch.permute(\n",
    "            model_input, (0, 3, 1, 2))[:, 0, :, :].to(device)  # ([B, XY, C, T] -> -> [B, T, XY, C] -> [B, XY, C]\n",
    "        ground_truth_up = torch.permute(\n",
    "            ground_truth_up, (0, 3, 1, 2)).to(device)  # [B, XY, C, T] -> [B, T, XY, C]\n",
    "        model_input_up = torch.permute(\n",
    "            model_input_up, (0, 3, 1, 2))[:, 0, :, :].to(device)  # ([B, XY, C, T] -> -> [B, T, XY, C] -> [B, XY, C]\n",
    "        \n",
    "        # On prend que la première grille (c'est tjs la mm dans deeponet) \n",
    "        b_size, t_size, hw_size, channels = ground_truth.shape\n",
    "\n",
    "        # t is T, model_input is B, T, XY, grid, ground_truth is B, T, XY, C\n",
    "\n",
    "        model_output = forward_deeponet_up(deeponet, ground_truth, ground_truth_up, coords, coords_up, timestamps, device)\n",
    "        print(\"model_output.shape, ground_truth_up.shape : \", model_output.shape, ground_truth_up.shape)\n",
    "        # B, T, XY, C\n",
    "        if n_frames_out == 0:\n",
    "            loss += criterion(model_output, ground_truth_up).item() * b_size\n",
    "        else : \n",
    "            loss = criterion(model_output, ground_truth_up).item() * b_size\n",
    "            loss_in_t = criterion(model_output[:, :n_frames_in, :, :], ground_truth_up[:, :n_frames_in, :, :]).item() * b_size\n",
    "            loss_out_t = criterion(model_output[:, n_frames_in:n_frames_in+n_frames_out, :, :], ground_truth_up[:, n_frames_in:n_frames_in+n_frames_out, :, :] ).item()* b_size\n",
    "            \n",
    "        if multichannel:\n",
    "            detailed_mse.aggregate(model_output.detach(),\n",
    "                                   ground_truth_up.detach())\n",
    "\n",
    "    loss /= n_seq\n",
    "    loss_in_t /= n_seq\n",
    "    loss_out_t /= n_seq\n",
    "\n",
    "    set_requires_grad(deeponet, True)\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        loss_in_t,\n",
    "        loss_out_t,\n",
    "    )\n",
    "\n",
    "\n",
    "def set_requires_grad(module, tf=False):\n",
    "    module.requires_grad = tf\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test...\n",
      "model_output.shape, ground_truth_up.shape :  torch.Size([8, 40, 2048, 2]) torch.Size([8, 40, 2048, 2])\n"
     ]
    }
   ],
   "source": [
    "# Out-of-domain evaluation\n",
    "print(\"Evaluating test...\")\n",
    "(\n",
    "    pred_test_mse,\n",
    "    pred_test_inter_mse,\n",
    "    pred_test_extra_mse\n",
    ") = eval_deeponet_up(\n",
    "    deeponet, \n",
    "    test_loader,\n",
    "    testset_2,\n",
    "    'cuda', \n",
    "    timestamps_ext,\n",
    "    criterion, \n",
    "    n_seq_test, \n",
    "    seq_inter_len, \n",
    "    seq_extra_len, \n",
    "    detailed_test_mse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01177526917308569 0.01664809323847294\n"
     ]
    }
   ],
   "source": [
    "print(pred_test_inter_mse, pred_test_extra_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_te_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.18e-2 & 1.66e-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.015738634392619133 0.019326908513903618"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
